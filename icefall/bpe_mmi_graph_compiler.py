import logging
from pathlib import Path
from typing import List, Tuple, Union

import k2
import sentencepiece as spm
import torch

from icefall.lexicon import Lexicon


class BpeMmiTrainingGraphCompiler(object):
    def __init__(
        self,
        lang_dir: Path,
        device: Union[str, torch.device] = "cpu",
        sos_token: str = "<sos/eos>",
        eos_token: str = "<sos/eos>",
    ) -> None:
        """
        Args:
          lang_dir:
            Path to the lang directory. It is expected to contain the
            following files::

                - tokens.txt
                - words.txt
                - bpe.model
                - P.fst.txt

            The above files are generated by the script `prepare.sh`. You
            should have run it before running the training code.

          device:
            It indicates CPU or CUDA.
          sos_token:
            The word piece that represents sos.
          eos_token:
            The word piece that represents eos.
        """
        self.lang_dir = Path(lang_dir)
        self.lexicon = Lexicon(lang_dir)
        self.device = device
        self.load_sentence_piece_model()
        self.build_ctc_topo_P()

        self.sos_id = self.sp.piece_to_id(sos_token)
        self.eos_id = self.sp.piece_to_id(eos_token)

        assert self.sos_id != self.sp.unk_id()
        assert self.eos_id != self.sp.unk_id()

    def load_sentence_piece_model(self) -> None:
        """Load the pre-trained sentencepiece model
        from self.lang_dir/bpe.model.
        """
        model_file = self.lang_dir / "bpe.model"
        sp = spm.SentencePieceProcessor()
        sp.load(str(model_file))
        self.sp = sp

    def build_ctc_topo_P(self):
        """Built ctc_topo_P, the composition result of
        ctc_topo and P, where P is a pre-trained bigram
        word piece LM.
        """
        # Note: there is no need to save a pre-compiled P and ctc_topo
        # as it is very fast to generate them.
        logging.info(f"Loading P from {self.lang_dir/'P.fst.txt'}")
        with open(self.lang_dir / "P.fst.txt") as f:
            # P is not an acceptor because there is
            # a back-off state, whose incoming arcs
            # have label #0 and aux_label 0 (i.e., <eps>).
            P = k2.Fsa.from_openfst(f.read(), acceptor=False)

        first_token_disambig_id = self.lexicon.token_table["#0"]

        # P.aux_labels is not needed in later computations, so
        # remove it here.
        del P.aux_labels
        # CAUTION: The following line is crucial.
        # Arcs entering the back-off state have label equal to #0.
        # We have to change it to 0 here.
        P.labels[P.labels >= first_token_disambig_id] = 0

        P = k2.remove_epsilon(P)
        P = k2.arc_sort(P)
        P = P.to(self.device)
        # Add epsilon self-loops to P because we want the
        # following operation "k2.intersect" to run on GPU.
        P_with_self_loops = k2.add_epsilon_self_loops(P)

        max_token_id = max(self.lexicon.tokens)
        logging.info(
            f"Building modified ctc_topo. max_token_id: {max_token_id}"
        )
        # CAUTION: We have to use a modifed version of CTC topo.
        # Otherwise, the resulting ctc_topo_P is so large that it gets
        # stuck in k2.intersect_dense_pruned() or it gets OOM in
        # k2.intersect_dense()
        ctc_topo = k2.ctc_topo(max_token_id, modified=True, device=self.device)

        ctc_topo_inv = k2.arc_sort(ctc_topo.invert_())

        logging.info("Building ctc_topo_P")
        ctc_topo_P = k2.intersect(
            ctc_topo_inv, P_with_self_loops, treat_epsilons_specially=False
        ).invert()

        self.ctc_topo_P = k2.arc_sort(ctc_topo_P)

    def texts_to_ids(self, texts: List[str]) -> List[List[int]]:
        """Convert a list of texts to a list-of-list of piece IDs.

        Args:
          texts:
            A list of transcripts. Within a transcript words are
            separated by spaces. An example input is::

                ['HELLO ICEFALL', 'HELLO k2']
        Returns:
          Return a list-of-list of piece IDs.
        """
        return self.sp.encode(texts, out_type=int)

    def compile(
        self, texts: List[str], replicate_den: bool = True
    ) -> Tuple[k2.Fsa, k2.Fsa]:
        """Create numerator and denominator graphs from transcripts.

        Args:
          texts:
            A list of transcripts. Within a transcript words are
            separated by spaces. An example input is::

                ["HELLO icefall", "HALLO WELT"]

          replicate_den:
            If True, the returned den_graph is replicated to match the number
            of FSAs in the returned num_graph; if False, the returned den_graph
            contains only a single FSA
        Returns:
          A tuple (num_graphs, den_graphs), where

            - `num_graphs` is the numerator graph. It is an FsaVec with
              shape `(len(texts), None, None)`.

            - `den_graphs` is the denominator graph. It is an FsaVec with the
              same shape of the `num_graph` if replicate_den is True;
              otherwise, it is an FsaVec containing only a single FSA.
        """
        token_ids = self.texts_to_ids(texts)
        token_fsas = k2.linear_fsa(token_ids, device=self.device)

        token_fsas_with_self_loops = k2.add_epsilon_self_loops(token_fsas)

        # NOTE: Use treat_epsilons_specially=False so that k2.compose
        # can be run on GPU
        num_graphs = k2.compose(
            self.ctc_topo_P,
            token_fsas_with_self_loops,
            treat_epsilons_specially=False,
        )
        # num_graphs may not be connected and
        # not be topologically sorted after k2.compose
        num_graphs = k2.connect(num_graphs)
        num_graphs = k2.top_sort(num_graphs)

        ctc_topo_P_vec = k2.create_fsa_vec([self.ctc_topo_P.detach()])
        if replicate_den:
            indexes = torch.zeros(
                len(texts), dtype=torch.int32, device=self.device
            )
            den_graphs = k2.index_fsa(ctc_topo_P_vec, indexes)
        else:
            den_graphs = ctc_topo_P_vec

        return num_graphs, den_graphs
